{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging A_R_Actor, P_Q_Actor, A_R_P_Q_Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use Keras Version 2.2.5. If you use tf.keras it causes some error. (In the tf.keras.backend.function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A_R_Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Sites and pages that are helpful:\n",
    "DDPG: https://spinningup.openai.com/en/latest/algorithms/ddpg.html\n",
    "DDPG Code: https://github.com/germain-hug/Deep-RL-Keras/tree/master/DDPG\n",
    "DDPG Code Updated: https://github.com/samhiatt/ddpg_agent/blob/master/ddpg_agent/agents/agent.py\n",
    "Keras.Gradient: https://www.tensorflow.org/api_docs/python/tf/gradients\n",
    "Keras.Function: https://www.tensorflow.org/api_docs/python/tf/keras/backend/function\n",
    "Zip: https://www.geeksforgeeks.org/zip-in-python/\n",
    "TensorFlow version used: less than 2.0.0 as then tf.gradients does work\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "\n",
    "\n",
    "class A_R_Actor:\n",
    "    '''\n",
    "        Actor Class for Accept Reject Network\n",
    "    '''\n",
    "\n",
    "    def __init__(self, inp_dim, lr, gaussian_std, out_dim):\n",
    "\n",
    "        self.inp_dim = inp_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.lr = lr\n",
    "        self.gaussian_std = gaussian_std\n",
    "        self.network()\n",
    "\n",
    "    def network(self):\n",
    "\n",
    "        inputs = layers.Input(shape=(self.inp_dim,))\n",
    "        #\n",
    "        x = layers.Dense(32, activation='relu')(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.GaussianNoise(self.gaussian_std)(x)\n",
    "        #\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.GaussianNoise(self.gaussian_std)(x)\n",
    "        #\n",
    "        outputs = layers.Dense(self.out_dim, activation='sigmoid')(x)\n",
    "        #\n",
    "        self.model = models.Model(input=inputs, output=outputs)\n",
    "\n",
    "        # Define loss function using action value (Q value) gradients\n",
    "        action_gradients = layers.Input(shape=(self.out_dim,))\n",
    "        self.test = -action_gradients * outputs\n",
    "        self.loss = K.mean(-action_gradients * outputs)\n",
    "        loss = K.mean(-action_gradients * outputs)\n",
    "\n",
    "        # Define optimizer and training function\n",
    "        optimizer = optimizers.Adam()\n",
    "        updates_op = optimizer.get_updates(params=self.model.trainable_weights, loss=loss)\n",
    "        self.train_fn = K.function(\n",
    "            inputs=[self.model.input, action_gradients, K.learning_phase()],\n",
    "            outputs=[loss],\n",
    "            updates=updates_op)\n",
    "\n",
    "    def summary(self):\n",
    "\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def action(self, state):\n",
    "\n",
    "        return self.model.predict(state)\n",
    "\n",
    "    def train(self, states, action_gradients):\n",
    "\n",
    "        #Grads will be supplied by the overall critic\n",
    "        return self.train_fn([states, action_gradients, 1])\n",
    "\n",
    "    def save(self, path):\n",
    "\n",
    "        self.model.save_weights(path + '_A_R_Actor.h5')\n",
    "\n",
    "    def load_weights(self, path):\n",
    "\n",
    "        self.model.load_weights(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P_Q_Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sites and pages that are helpful:\n",
    "DDPG: https://spinningup.openai.com/en/latest/algorithms/ddpg.html\n",
    "DDPG Code: https://github.com/germain-hug/Deep-RL-Keras/tree/master/DDPG\n",
    "DDPG Code Updated: https://github.com/samhiatt/ddpg_agent/blob/master/ddpg_agent/agents/agent.py\n",
    "Keras.Gradient: https://www.tensorflow.org/api_docs/python/tf/gradients\n",
    "Keras.Function: https://www.tensorflow.org/api_docs/python/tf/keras/backend/function\n",
    "Zip: https://www.geeksforgeeks.org/zip-in-python/\n",
    "TensorFlow version used: less than 2.0.0 as then tf.gradients does work\n",
    "'''\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "class P_Q_Actor:\n",
    "    '''\n",
    "        Actor Class for Accept Reject Network\n",
    "    '''\n",
    "\n",
    "    def __init__(self, inp_dim, lr, gaussian_std, out_dim):\n",
    "        self.inp_dim = inp_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.lr = lr\n",
    "        self.gaussian_std = gaussian_std\n",
    "        self.network()\n",
    "\n",
    "    def network(self):\n",
    "        inputs = layers.Input(shape=(self.inp_dim,))\n",
    "        #\n",
    "        x = layers.Dense(32, activation='relu')(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.GaussianNoise(self.gaussian_std)(x)\n",
    "        #\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.GaussianNoise(self.gaussian_std)(x)\n",
    "        #\n",
    "        output_q_traded = layers.Dense(1, activation='tanh')(x)\n",
    "        output_q = layers.Dense(self.out_dim, activation='softmax')(x)\n",
    "        output_p = layers.Dense(self.out_dim, activation='tanh')(x)\n",
    "        #\n",
    "        self.model = models.Model(input=inputs, output=[output_p, output_q, output_q_traded])\n",
    "        '''\n",
    "            output_q_traded: is a portion of the total energy demanded or supplied by the D_S_Net\n",
    "            output_q: is the softmax (distribution) of energy amongst buyers and sellers\n",
    "            output_p: is the price at which the trade will occur\n",
    "        '''\n",
    "\n",
    "        p_act_grad = layers.Input(shape=(self.out_dim,))\n",
    "        q_act_grad = layers.Input(shape=(self.out_dim,))\n",
    "        q_traded_grad = layers.Input(shape=(1,))\n",
    "\n",
    "        self.loss_1 = p_act_grad * output_p\n",
    "        self.loss_2 = q_act_grad * output_q\n",
    "        self.loss_3 = q_traded_grad * output_q_traded\n",
    "        loss = K.mean(- p_act_grad * output_p) + K.mean(-q_act_grad * output_q) + K.mean(-q_traded_grad * output_q_traded)\n",
    "\n",
    "        # Define optimizer and training function\n",
    "        optimizer = optimizers.Adam()\n",
    "        updates_op = optimizer.get_updates(params=self.model.trainable_weights, loss=loss)\n",
    "        self.train_fn = K.function(\n",
    "            inputs=[self.model.input, p_act_grad, q_act_grad, q_traded_grad, K.learning_phase()],\n",
    "            outputs=[loss],\n",
    "            updates=updates_op)\n",
    "\n",
    "    def summary(self):\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def action(self, state):\n",
    "        return self.model.predict(state)\n",
    "\n",
    "    def train(self, states, p_grads, q_grads,  q_traded_grads):\n",
    "        # Grads will be supplied by the overall critic\n",
    "        return self.train_fn([states, p_grads, q_grads, q_traded_grads, 1])\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.save_weights(path + '_P_Q_Actor.h5')\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        self.model.load_weights(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A_R_P_Q_Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sites and pages that are helpful:\n",
    "DDPG: https://spinningup.openai.com/en/latest/algorithms/ddpg.html\n",
    "DDPG Code: https://github.com/germain-hug/Deep-RL-Keras/tree/master/DDPG\n",
    "DDPG Code Updated: https://github.com/samhiatt/ddpg_agent/blob/master/ddpg_agent/agents/agent.py\n",
    "Keras.Gradient: https://www.tensorflow.org/api_docs/python/tf/gradients\n",
    "Keras.Function: https://www.tensorflow.org/api_docs/python/tf/keras/backend/function\n",
    "Zip: https://www.geeksforgeeks.org/zip-in-python/\n",
    "TensorFlow version used: less than 2.0.0 as then tf.gradients does work\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "\n",
    "\n",
    "class A_R_P_Q_Critic:\n",
    "\n",
    "    def __init__(self, a_r_inp_dim, p_q_inp_dim, a_r_act_dim, p_act_dim, q_act_dim, q_traded_act_dim=1):\n",
    "        self.a_r_inp_dim = a_r_inp_dim\n",
    "        self.p_q_inp_dim = p_q_inp_dim\n",
    "        self.a_r_act_dim = a_r_act_dim\n",
    "        self.p_act_dim = p_act_dim\n",
    "        self.q_act_dim = q_act_dim\n",
    "        self.q_traded_act_dim = q_traded_act_dim\n",
    "        self.network()\n",
    "\n",
    "    def network(self):\n",
    "\n",
    "        a_r_inp = layers.Input(shape=(self.a_r_inp_dim,))\n",
    "        p_q_inp = layers.Input(shape=(self.p_q_inp_dim,))\n",
    "        a_r_act_inp = layers.Input(shape=(self.a_r_act_dim,))\n",
    "        p_act_inp = layers.Input(shape=(self.p_act_dim,))\n",
    "        q_act_inp = layers.Input(shape=(self.q_act_dim,))\n",
    "        q_traded_act_inp = layers.Input(shape=(self.q_traded_act_dim,))\n",
    "        #\n",
    "        x = layers.concatenate([a_r_inp, p_q_inp, a_r_act_inp, p_act_inp, q_act_inp, q_traded_act_inp], axis=-1)\n",
    "        x = layers.Dense(32, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        #\n",
    "        x = layers.Dense(32, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        #\n",
    "        output = layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "        self.model = models.Model(inputs=[a_r_inp, p_q_inp, a_r_act_inp, p_act_inp, q_act_inp, q_traded_act_inp], outputs=output)\n",
    "\n",
    "        optimizer = optimizers.Adam()\n",
    "        self.model.compile(optimizer=optimizer, loss='mse')\n",
    "        \n",
    "        self.get_action_gradients = K.function( inputs= [self.model.input[0], self.model.input[1], self.model.input[2],\n",
    "                                                       self.model.input[3], self.model.input[4], self.model.input[5], K.learning_phase()],\n",
    "                                                      outputs = K.gradients(self.model.output,\n",
    "                                                                                 [self.model.input[2],\n",
    "                                                                                  self.model.input[3],\n",
    "                                                                                  self.model.input[4],\n",
    "                                                                                  self.model.input[5]]))\n",
    "\n",
    "\n",
    "    def gradients(self, a_r_inp, p_q_inp, a_r_act_inp, p_act_inp, q_act_inp, q_traded_act_inp):\n",
    "\n",
    "        return self.get_action_gradients([a_r_inp, p_q_inp, a_r_act_inp, p_act_inp, q_act_inp, q_traded_act_inp, 0])\n",
    "\n",
    "    def reward_value(self, a_r_inp, p_q_inp, a_r_act_inp, p_act_inp, q_act_inp, q_traded_act_inp):\n",
    "\n",
    "        return self.model.predict([a_r_inp, p_q_inp, a_r_act_inp, p_act_inp, q_act_inp, q_traded_act_inp])\n",
    "\n",
    "    def train(self, a_r_inp, p_q_inp, a_r_act_inp, p_act_inp, q_act_inp, q_traded_act_inp,\n",
    "              a_r_inp_next, p_q_inp_next, a_r_act_inp_next, p_act_inp_next, q_act_inp_next, q_traded_act_inp_next,\n",
    "              total_rewards):\n",
    "      \n",
    "        y = self.model.predict([a_r_inp_next, p_q_inp_next, a_r_act_inp_next, p_act_inp_next, q_act_inp_next, q_traded_act_inp_next])\n",
    "\n",
    "        for i in range(len(total_rewards)):\n",
    "            if total_rewards[i] == [0.0]:\n",
    "                y[i] = total_rewards[i]\n",
    "\n",
    "        loss = self.model.train_on_batch(x=[a_r_inp, p_q_inp, a_r_act_inp, p_act_inp, q_act_inp, q_traded_act_inp], y=y)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def save(self, path):\n",
    "\n",
    "        self.model.save_weights(path + '_A_R_P_Q_Critic.h5')\n",
    "\n",
    "    def load_weights(self, path):\n",
    "\n",
    "        self.model.load_weights(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging the A_R_P_Q_Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A_R_P_Q_Critic_ = A_R_P_Q_Critic(5, 5, 6, 7, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "a_r_inp = np.array([[1,2,3,4,5],[1,2,3,4,5]]).astype(np.float32)\n",
    "p_q_inp = np.array([[1,2,3,4,5],[1,2,3,4,5]]).astype(np.float32)\n",
    "a_r_act = np.array([[1,2,3,4,5,6],[1,2,3,4,5,6]]).astype(np.float32).reshape(-1, 6)\n",
    "p_act = np.array([[1,2,3,4,5,6,7],[1,2,3,4,5,6,7]]).astype(np.float32).reshape(-1, 7)\n",
    "q_act = np.array([[1,2,3,4,5,6,7], [1,2,3,4,5,6,7]]).astype(np.float32).reshape(-1, 7)\n",
    "q_traded = np.array([[1],[2]]).reshape(-1,1)\n",
    "reward_1 = np.array([[1], [1]]).reshape(-1,1)\n",
    "reward_0 = np.array([[0], [0]]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[-0.09060257,  0.05156034,  0.04906118, -0.19119526, -0.15446454,\n",
       "         -0.04544139],\n",
       "        [-0.09060257,  0.05156034,  0.04906118, -0.19119526, -0.15446454,\n",
       "         -0.04544139]], dtype=float32),\n",
       " array([[ 0.14027981, -0.20565191,  0.12181181,  0.06962101,  0.08803711,\n",
       "         -0.07280227,  0.1196762 ],\n",
       "        [ 0.14027981, -0.20565191,  0.12181181,  0.06962101,  0.08803711,\n",
       "         -0.07280227,  0.1196762 ]], dtype=float32),\n",
       " array([[-0.04523172, -0.04033513, -0.23001516, -0.0611654 , -0.2211808 ,\n",
       "          0.18053871, -0.09471793],\n",
       "        [-0.04523172, -0.04033513, -0.23001516, -0.0611654 , -0.2211808 ,\n",
       "          0.18053871, -0.09471793]], dtype=float32),\n",
       " array([[-0.05013109],\n",
       "        [-0.05013109]], dtype=float32)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradients\n",
    "A_R_P_Q_Critic_.gradients(a_r_inp, p_q_inp, a_r_act, p_act, q_act, q_traded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.623348 ],\n",
       "       [-1.6734791]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reward Value\n",
    "A_R_P_Q_Critic_.reward_value(a_r_inp, p_q_inp, a_r_act, p_act, q_act, q_traded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.39946878"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "A_R_P_Q_Critic_.train(a_r_inp, p_q_inp, a_r_act, p_act, q_act, q_traded,a_r_inp, p_q_inp, a_r_act, p_act, q_act, q_traded, reward_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116.73743"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "A_R_P_Q_Critic_.train(a_r_inp, p_q_inp, a_r_act, p_act, q_act, q_traded,a_r_inp, p_q_inp, a_r_act, p_act, q_act, q_traded,reward_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Individual Gradients\n",
    "[a_r_grad, p_grad, q_grad, q_traded_grad] = A_R_P_Q_Critic_.gradients(a_r_inp, p_q_inp, a_r_act, p_act, q_act, q_traded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging the A_R_Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\ipykernel_launcher.py:48: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "A_R_Actor_ = A_R_Actor(5, 0.1, 0.1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3539759 , 0.04806849, 0.6060275 , 0.52506167, 0.20672342,\n",
       "        0.75820374],\n",
       "       [0.3539759 , 0.04806849, 0.6060275 , 0.52506167, 0.20672342,\n",
       "        0.75820374]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action\n",
    "A_R_Actor_.action(a_r_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.6140823]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradients\n",
    "A_R_Actor_.train(a_r_inp, a_r_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mul:0' shape=(?, 6) dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### To understand whats happening internally\n",
    "A_R_Actor_.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mean:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### To understand whats happening internally\n",
    "A_R_Actor_.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### To understand whats happening internally\n",
    "np.array([1, 2]) * np.array([3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging the P_Q_Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\ipykernel_launcher.py:50: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "P_Q_Actor_ = P_Q_Actor(5, 0.1, 0.01, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.12405473,  0.37628353, -0.07276691,  0.69627243,  0.9998546 ,\n",
       "          0.06685907, -0.6633774 ],\n",
       "        [ 0.12405473,  0.37628353, -0.07276691,  0.69627243,  0.9998546 ,\n",
       "          0.06685907, -0.6633774 ]], dtype=float32),\n",
       " array([[0.02626349, 0.01357553, 0.00766519, 0.8655479 , 0.07685392,\n",
       "         0.0026146 , 0.00747942],\n",
       "        [0.02626349, 0.01357553, 0.00766519, 0.8655479 , 0.07685392,\n",
       "         0.0026146 , 0.00747942]], dtype=float32),\n",
       " array([[-0.79066193],\n",
       "        [-0.79066193]], dtype=float32)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action\n",
    "P_Q_Actor_.action(a_r_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Individual Actions\n",
    "[p_output, q_output, q_traded_output] = P_Q_Actor_.action(a_r_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.8229717]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "P_Q_Actor_.train(a_r_inp, p_grad, q_grad, q_traded_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if the training does cause an improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.12606578,  0.38212767, -0.06446828,  0.6932459 ,  0.99985516,\n",
       "          0.06563277, -0.6660674 ],\n",
       "        [ 0.12606578,  0.38212767, -0.06446828,  0.6932459 ,  0.99985516,\n",
       "          0.06563277, -0.6660674 ]], dtype=float32),\n",
       " array([[0.02639188, 0.01365938, 0.00771181, 0.8647045 , 0.07742614,\n",
       "         0.00264839, 0.00745786],\n",
       "        [0.02639188, 0.01365938, 0.00771181, 0.8647045 , 0.07742614,\n",
       "         0.00264839, 0.00745786]], dtype=float32),\n",
       " array([[-0.7834161],\n",
       "        [-0.7834161]], dtype=float32)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action\n",
    "P_Q_Actor_.action(a_r_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.2958459]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "P_Q_Actor_.train(a_r_inp, p_grad, q_grad, q_traded_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.12033914,  0.38290933, -0.05938209,  0.69127613,  0.9998548 ,\n",
       "          0.06264965, -0.66897184],\n",
       "        [ 0.12033914,  0.38290933, -0.05938209,  0.69127613,  0.9998548 ,\n",
       "          0.06264965, -0.66897184]], dtype=float32),\n",
       " array([[0.02657991, 0.01369545, 0.00770971, 0.8645353 , 0.07735843,\n",
       "         0.00266778, 0.00745341],\n",
       "        [0.02657991, 0.01369545, 0.00770971, 0.8645353 , 0.07735842,\n",
       "         0.00266778, 0.00745341]], dtype=float32),\n",
       " array([[-0.7777392],\n",
       "        [-0.7777392]], dtype=float32)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action\n",
    "P_Q_Actor_.action(a_r_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mul_44:0' shape=(?, 7) dtype=float32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### To understand whats happening internally\n",
    "P_Q_Actor_.loss_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mul_45:0' shape=(?, 7) dtype=float32>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### To understand whats happening internally\n",
    "P_Q_Actor_.loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mul_46:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### To understand whats happening internally\n",
    "P_Q_Actor_.loss_3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
