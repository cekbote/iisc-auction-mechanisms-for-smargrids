{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging D_S_Actor and D_S_Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use Keras Version 2.2.5. If you use tf.keras it causes some error. (In the tf.keras.backend.function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n",
      "2.2.4-tf\n",
      "2.2.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "print(tensorflow.__version__)\n",
    "print(tensorflow.keras.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D_S_Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sites and pages that are helpful:\n",
    "DDPG: https://spinningup.openai.com/en/latest/algorithms/ddpg.html\n",
    "DDPG Code: https://github.com/germain-hug/Deep-RL-Keras/tree/master/DDPG\n",
    "DDPG Code Updated: https://github.com/samhiatt/ddpg_agent/blob/master/ddpg_agent/agents/agent.py\n",
    "Keras.Gradient: https://www.tensorflow.org/api_docs/python/tf/gradients\n",
    "Keras.Function: https://www.tensorflow.org/api_docs/python/tf/keras/backend/function\n",
    "Zip: https://www.geeksforgeeks.org/zip-in-python/\n",
    "TensorFlow version used: less than 2.0.0 as then tf.gradients dosent work\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "\n",
    "class D_S_Actor:\n",
    "\n",
    "    def __init__(self, state_dim, lr, gaussian_std, act_dim=1):\n",
    "        self.state_dim = state_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.lr = lr\n",
    "        self.gaussian_std = gaussian_std\n",
    "        \n",
    "        self.network()\n",
    "#         self.optimizer = self.optim()\n",
    "\n",
    "    def network(self):\n",
    "        states = layers.Input(shape= (self.state_dim,))\n",
    "        #\n",
    "        x = layers.Dense(32, activation='relu')(states)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.GaussianNoise(self.gaussian_std)(x)\n",
    "        #\n",
    "        x = layers.Dense(32, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.GaussianNoise(self.gaussian_std)(x)\n",
    "        #\n",
    "        actions = layers.Dense(self.act_dim, activation='tanh')(x)\n",
    "        #\n",
    "        self.model = models.Model(inputs=states, outputs=actions)\n",
    "        \n",
    "        # Define loss function using action value (Q value) gradients\n",
    "        action_gradients = layers.Input(shape=(self.act_dim,))\n",
    "        loss = K.mean(-action_gradients * actions)\n",
    "\n",
    "        # Define optimizer and training function\n",
    "        optimizer = optimizers.Adam()\n",
    "        updates_op = optimizer.get_updates(params=self.model.trainable_weights, loss=loss)\n",
    "        self.train_fn = K.function(\n",
    "            inputs=[self.model.input, action_gradients, K.learning_phase()],\n",
    "            outputs=[loss],\n",
    "            updates=updates_op)\n",
    "\n",
    "    def summary(self):\n",
    "\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def action(self, state):\n",
    "\n",
    "        return self.model.predict(state)\n",
    "    \n",
    "    def train(self, state, action_gradients):\n",
    "        \n",
    "        return self.train_fn([state, action_gradients, 1])\n",
    "\n",
    "    def save(self, path):\n",
    "\n",
    "        self.model.save_weights(path + '_D_S_Actor.h5')\n",
    "\n",
    "    def load_weights(self, path):\n",
    "\n",
    "        self.model.load_weights(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D_S_Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sites and pages that are helpful:\n",
    "DDPG: https://spinningup.openai.com/en/latest/algorithms/ddpg.html\n",
    "DDPG Code: https://github.com/germain-hug/Deep-RL-Keras/tree/master/DDPG\n",
    "DDPG Code Updated: https://github.com/samhiatt/ddpg_agent/blob/master/ddpg_agent/agents/agent.py\n",
    "Keras.Gradient: https://www.tensorflow.org/api_docs/python/tf/gradients\n",
    "Keras.Function: https://www.tensorflow.org/api_docs/python/tf/keras/backend/function\n",
    "Zip: https://www.geeksforgeeks.org/zip-in-python/\n",
    "TensorFlow version used: less than 2.0.0 as then tf.gradients dosent work\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "\n",
    "class D_S_Critic:\n",
    "\n",
    "    def __init__(self, state_dim, act_dim):\n",
    "        self.gamma = 0.8\n",
    "        self.state_dim = state_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.model = self.network()\n",
    "        self.network()\n",
    "        \n",
    "    def network(self):\n",
    "        state_inp = layers.Input(shape = (self.state_dim, ))\n",
    "\n",
    "        action_inp = layers.Input(shape = (self.act_dim, ))\n",
    "        #\n",
    "        x = layers.concatenate([state_inp, action_inp])\n",
    "        x = layers.Dense(32, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        #\n",
    "        x = layers.Dense(32, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        #\n",
    "        output = layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "        self.model = models.Model(inputs=[state_inp, action_inp], outputs=output)\n",
    "        \n",
    "        optimizer = optimizers.Adam()\n",
    "        self.model.compile(optimizer=optimizer, loss='mse')\n",
    "        \n",
    "        # Define an additional function to fetch action gradients (to be used by actor model)\n",
    "        self.get_action_gradients = K.function(\n",
    "            inputs=[self.model.input[0], self.model.input[1], K.learning_phase()],\n",
    "            outputs= K.gradients(self.model.output,  [self.model.input[1]]))\n",
    "\n",
    "    def gradients(self, state_inp, action_inp):\n",
    "\n",
    "        return self.get_action_gradients([state_inp, action_inp, 0])\n",
    "\n",
    "    def reward_value(self, state_inp, action_inp):\n",
    "\n",
    "        return self.model.predict([state_inp, action_inp])\n",
    "\n",
    "    def train(self, state_inp, action_inp, state_inp_next, action_inp_next, total_rewards):\n",
    "\n",
    "        y = total_rewards + self.gamma*self.model.predict([state_inp_next, action_inp_next])\n",
    "        loss = self.model.train_on_batch(x=[state_inp, action_inp], y=y)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def save(self, path):\n",
    "\n",
    "        self.model.save_weights(path + '_D_S_Critic.h5')\n",
    "\n",
    "    def load_weights(self, path):\n",
    "\n",
    "        self.model.load_weights(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Actor and Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_S_Actor_ = D_S_Actor(5, 0.01, 0.01)\n",
    "D_S_Critic_ = D_S_Critic(5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging the Critic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3., 4., 5.]], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = np.array([[1,2,3,4,5]]).astype(np.float32)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = np.array([[1]]).astype(np.float32).reshape(-1, 1) \n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[84.712944]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# State is np.asarray([[1,2,3,4,5]])\n",
    "# Action is np.array([[1]])\n",
    "grads = D_S_Critic_.gradients(state, action)  # Outputs gradient w.r.t actions\n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00845966]], dtype=float32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_S_Critic_.reward_value(np.asarray([[1,2,3,4,5]]), np.array([[1]]))  # Outputs reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60063.895"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next State is np.asarray([[3,5,3,3,2]]\n",
    "# Next Action is np.array([[2]]\n",
    "# Reward is np.array([[5]])\n",
    "# Function outputs loss\n",
    "D_S_Critic_.train(np.asarray([[1,2,3,4,5]]), np.array([[1]]), np.asarray([[3,5,3,3,2]]), np.array([[2]]), np.array([[5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging the Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36620203]], dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_S_Actor_.model.predict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[84.9182]], dtype=float32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_gradients = np.reshape(D_S_Critic_.gradients(state, action), (-1,1))\n",
    "action_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if the Gradient works by looking at a change in the action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36620203]], dtype=float32)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_S_Actor_.model.predict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2.8813336]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_S_Actor_.train(state, action_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37086245]], dtype=float32)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_S_Actor_.model.predict(state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
