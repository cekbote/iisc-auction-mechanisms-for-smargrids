{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging_Replay_Buffers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay_Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sites and pages that are helpful:\n",
    "DDPG: https://spinningup.openai.com/en/latest/algorithms/ddpg.html\n",
    "DDPG Code: https://github.com/germain-hug/Deep-RL-Keras/tree/master/DDPG\n",
    "DDPG Code Updated: https://github.com/samhiatt/ddpg_agent/blob/master/ddpg_agent/agents/agent.py\n",
    "Keras.Gradient: https://www.tensorflow.org/api_docs/python/tf/gradients\n",
    "Keras.Function: https://www.tensorflow.org/api_docs/python/tf/keras/backend/function\n",
    "Zip: https://www.geeksforgeeks.org/zip-in-python/\n",
    "TensorFlow version used: less than 2.0.0 as then tf.gradients does work\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, action_dim, ):\n",
    "\n",
    "        self.states = collections.deque(maxlen = 10000)\n",
    "        self.actions = collections.deque(maxlen = 10000)\n",
    "        self.rewards = collections.deque(maxlen = 10000)\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def store_transition_state(self, state):\n",
    "        self.states.append(state)\n",
    "\n",
    "    def store_transition_action(self, action):\n",
    "        self.actions.append(action)\n",
    "\n",
    "    def store_transition_reward(self, reward):\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "\n",
    "        batch = np.random.choice(9999, batch_size)\n",
    "        \n",
    "        current_states = np.array([self.states[i] for i in batch]).astype(np.float32)\n",
    "        actions = np.array([self.actions[i] for i in batch]).astype(np.float32).reshape(-1, self.action_dim)\n",
    "        rewards = np.array([self.rewards[i] for i in batch]).astype(np.float32).reshape(-1, 1)\n",
    "        next_states = np.array([self.states[i] for i in (batch + 1)]).astype(np.float32)\n",
    "        next_actions = np.array([self.actions[i] for i in (batch + 1)]).astype(np.float32)\n",
    "\n",
    "        return current_states, actions, rewards, next_states, next_actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReplayBuffer_ = ReplayBuffer(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_transition_state\n",
    "ReplayBuffer_.store_transition_state(np.array([1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_transition_action\n",
    "ReplayBuffer_.store_transition_action(np.array([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_transition_reward\n",
    "ReplayBuffer_.store_transition_reward(np.array([4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling, queue is not full, hence it doesent work \n",
    "# ReplayBuffer_.sample_buffer(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReplayBuffer_A_R_P_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sites and pages that are helpful:\n",
    "DDPG: https://spinningup.openai.com/en/latest/algorithms/ddpg.html\n",
    "DDPG Code: https://github.com/germain-hug/Deep-RL-Keras/tree/master/DDPG\n",
    "DDPG Code Updated: https://github.com/samhiatt/ddpg_agent/blob/master/ddpg_agent/agents/agent.py\n",
    "Keras.Gradient: https://www.tensorflow.org/api_docs/python/tf/gradients\n",
    "Keras.Function: https://www.tensorflow.org/api_docs/python/tf/keras/backend/function\n",
    "Zip: https://www.geeksforgeeks.org/zip-in-python/\n",
    "TensorFlow version used: less than 2.0.0 as then tf.gradients does work\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "\n",
    "\n",
    "class ReplayBuffer_A_R_P_Q:\n",
    "\n",
    "    def __init__(self, a_r_dim, p_dim, q_dim, q_traded_dim):\n",
    "        self.states_a_r = collections.deque(maxlen=10000)\n",
    "        self.states_p_q = collections.deque(maxlen=10000)\n",
    "        self.actions_a_r = collections.deque(maxlen=10000)\n",
    "        self.actions_p = collections.deque(maxlen=10000)\n",
    "        self.actions_q = collections.deque(maxlen=10000)\n",
    "        self.actions_q_traded = collections.deque(maxlen=10000)\n",
    "        self.rewards = collections.deque(maxlen=10000)\n",
    "        self.via_matrix = collections.deque(maxlen=10000)\n",
    "        self.customized_status = collections.deque(maxlen=10000)\n",
    "        \n",
    "        self.a_r_dim = a_r_dim\n",
    "        self.p_dim = p_dim\n",
    "        self.q_dim = q_dim\n",
    "        self.q_traded = q_traded_dim\n",
    "\n",
    "    def store_transition_state_a_r(self, state_a_r):\n",
    "        self.states_a_r.append(state_a_r)\n",
    "\n",
    "    def store_transition_state_p_q(self, state_p_q):\n",
    "        self.states_p_q.append(state_p_q)\n",
    "\n",
    "    def store_transition_action_a_r(self, action_a_r):\n",
    "        self.actions_a_r.append(action_a_r)\n",
    "\n",
    "    def store_transition_action_p_q(self, action_p, action_q, action_q_traded):\n",
    "        self.actions_p.append(action_p)\n",
    "        self.actions_q.append(action_q)\n",
    "        self.actions_q_traded.append(action_q_traded)\n",
    "\n",
    "    def store_transition_reward(self, reward):\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def store_transition_via_matrix(self, via_matrix):\n",
    "        self.via_matrix.append(via_matrix)\n",
    "\n",
    "    def store_transition_customized_status(self, customized_status):\n",
    "        self.customized_status.append(customized_status)\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        \n",
    "        batch = np.random.choice(9999, batch_size)\n",
    "\n",
    "        current_states_a_r = np.array([self.states_a_r[i] for i in batch]).astype(np.float32)\n",
    "        current_states_p_q = np.array([self.states_p_q[i] for i in batch]).astype(np.float32)\n",
    "        actions_a_r = np.array([self.actions_a_r[i] for i in batch]).astype(np.float32).reshape(-1, self.a_r_dim)\n",
    "        actions_p = np.array([self.actions_p[i] for i in batch]).astype(np.float32).reshape(-1, self.p_dim)\n",
    "        actions_q = np.array([self.actions_q[i] for i in batch]).astype(np.float32).reshape(-1, self.q_dim)\n",
    "        actions_q_traded = np.array([self.actions_q_traded[i] for i in batch]).astype(np.float32).reshape(-1, self.q_traded_dim)\n",
    "\n",
    "        rewards = np.array([self.rewards[i] for i in batch])\n",
    "\n",
    "        next_states_a_r = np.array([self.states_a_r[i] for i in (batch + 1)]).astype(np.float32)\n",
    "        next_states_p_q = np.array([self.states_p_q[i] for i in (batch + 1)]).astype(np.float32)\n",
    "        next_actions_a_r = np.array([self.actions_a_r[i] for i in (batch + 1)]).astype(np.float32).reshape(-1, self.a_r_dim)\n",
    "        next_actions_p = np.array([self.actions_p[i] for i in (batch + 1)]).astype(np.float32).reshape(-1, self.p_dim)\n",
    "        next_actions_q = np.array([self.actions_q[i] for i in (batch + 1)]).astype(np.float32).reshape(-1, self.q_dim)\n",
    "        next_actions_q_traded = np.array([self.actions_q_traded[i] for i in (batch + 1)]).astype(np.float32).reshape(-1, self.q_traded_dim)\n",
    "\n",
    "        via_matrix = np.array([self.via_matrix[i] for i in batch]).astype(np.float32)\n",
    "        customized_status = np.array([self.customized_status[i] for i in batch]).astype(np.float32)\n",
    "\n",
    "        next_via_matrix = np.array([self.via_matrix[i] for i in (batch + 1)]).astype(np.float32)\n",
    "        next_customized_status = np.array([self.customized_status[i] for i in (batch + 1)]).astype(np.float32)\n",
    "\n",
    "        return current_states_a_r, current_states_p_q, actions_a_r, actions_p, actions_q, actions_q_traded, rewards, next_states_a_r, \\\n",
    "               next_states_p_q, next_actions_a_r, next_actions_p, next_actions_q, next_actions_q_traded, via_matrix, customized_status, \\\n",
    "               next_via_matrix, next_customized_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The 2nd Code is derived from the first, hence the assumption is that there is nothing wrong with it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
