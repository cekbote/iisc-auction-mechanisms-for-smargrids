"""Main Code DDPG.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1ICpQMHsxWbYZKJK87R2JxebyRSUxf0pv
"""

'''
Sites and pages that are helpful:
DDPG: https://spinningup.openai.com/en/latest/algorithms/ddpg.html
DDPG Code: https://github.com/germain-hug/Deep-RL-Keras/tree/master/DDPG
Keras.Gradient: https://www.tensorflow.org/api_docs/python/tf/gradients
Keras.Function: https://www.tensorflow.org/api_docs/python/tf/keras/backend/function
Zip: https://www.geeksforgeeks.org/zip-in-python/
TensorFlow version used: less than 2.0.0 as then tf.gradients dosent work
'''

# Classes
from ADL_Actor import ADL_Actor
from ADL_Critic import ADL_Critic
from D_S_Actor import D_S_Actor
from D_S_Critic import D_S_Critic
from A_R_Actor import A_R_Actor
from P_Q_Actor import P_Q_Actor
from A_R_P_Q_Critic import A_R_P_Q_Critic
from ReplayBuffer import ReplayBuffer
from ReplayBuffer_A_R_P_Q import ReplayBuffer_A_R_P_Q

# Libraries
import collections
import random
import numpy as np
import math
import copy
import logging

'''
    To Dos
    Adding Gamma to all rewards for the Critics
    Add arguments to all the classes in the dictionary
    Writing the get_renewable and update_adl function and get_demand function
    Create the Q(s1,s2,a1,a2,a3,a4) - a1'[Loss Values] for the A_R_Network 
    Incorporate ADL
    Use the inverse tanh function for the linear transformation.  DONE. GET IT CHECKED.
    Modify Replay Buffer Class so that you dont have to store the next state seperately. DONE
'''

'''
    For the implementation of a linear curve fit for the tanh function, we assume the highest values are:
    np.arctanh(0.9999999999999999)= 18.714973875118524 & np.arctanh(-0.9999999999999999) = -18.714973875118524
    And the assumption is those many digits are only going to be in the output (No idea if this is true)
'''


def get_renewable(index, time):
    lamb = [[2.667e-07, 0.541, 6.5965, 4.3712],[8.0875, 8.62, 8.7815, 8.80],[8.0875, 8.62, 8.7815, 8.80]]
    energy = np.random.poisson(lam=lamb[index][time], size=1)
    energy = min([10, energy])  # clipping the value so that it can't exceede max_battery
    energy = int(math.floor(energy))
    return energy


# Write this function
def get_demand(index, time):
    non_adl = [3, 4, 5, 6]
    prob_non_adl = [[0.4,0.3,0.2,0.1],[0.1,0.4,0.3,0.2],[0.1,0.3,0.4,0.2],[0.2,0.3,0.1,0.4]]
    demand = np.random.choice(non_adl, p=prob_non_adl[time - 1])
    return int(demand)


# Write this function
def update_adl(index, time):
    return 0


formatter = logging.Formatter('%(message)s')


def setup_logger(name, log_file, level=logging.INFO):
    """Function setup as many loggers as you want"""
    handler = logging.FileHandler(log_file)
    handler.setFormatter(formatter)

    logger = logging.getLogger(name)
    logger.setLevel(level)
    logger.addHandler(handler)

    return logger


def main():
    '''
        States and Actions of the main controller:
            ADL:
                States: [ time, battery, renewable-energy, ADL_State, current-demand, grid-price]
                ACtions: ADL-Demand to be fulfilled in that time step
            D_S:
                States: [ time, battery, renewable-energy, ADL_State, current-demand, grid-price, ADL_action]
                Actions: Energy demand or energy supply that has to performed at that time step
        States and Actions of the Sub-Controllers (Roughly):
            A_R:
                States: [ internal time, n-sized vector that indicates whose a buyer or seller, p and q for all the other agents]
                Actions: Accept or Reject Vector
            P_Q:
                States:  [ internal time, n-sized vector that indicates whose a buyer or seller, Accept_reject vector, p and q for all the other agents, ]
                Actions: Price and Quantity Vector
    '''

    # Loggers
    reward_logger = setup_logger('reward_logger', './logs/reward_logger.txt')
    losses_logger = setup_logger('losses_logger', './logs/losses_logger.txt')
    sub_transaction = setup_logger('sub_transaction_logger', './logs/sub_transaction_logger.txt')
    main_transaction = setup_logger('main_transaction_logger', './logs/main_transaction_logger.txt')
    energy_transfered = setup_logger('energy_transfered_logger', './logs/energy_transfered.txt')
    actions_log = setup_logger('actions', './logs/actions.txt')
    price_log = setup_logger('price_logger', './logs/price.txt')

    no_of_agents = 3
    k = 5
    grid_price = 20
    lower_price = grid_price - 5
    names = ['Agent_1', 'Agent_2', 'Agent_3', 'Agent_4', 'Agent_5']  # Names for the agents
    agent = []
    max_battery = 10
    total_no_of_iterations = 200000
    time_steps_per_day = 4
    no_of_sub_transactions = 4
    c = 30
    clip = 1  # was 1000
    clip_1 = 1  # was 3
    D_S_constraints = [1, 1, 0]  # Write the appropriate constraints for each agent before running. Controls whether its intelligent or not.
    P_Q_constraints = [1, 1, 0]  # Write the appropriate constraints for each agent before running. Controls whether its intelligent or not.
    A_R_constraints = [1, 1, 0]  # Write the appropriate constraints for each agent before running. Controls whether its intelligent or not.
    energy_demand = np.zeros(no_of_agents)
    energy_transacted_micro = np.zeros(no_of_agents)
    energy_transacted_central = np.zeros(no_of_agents)
    buffer_size = 5000 # Change, make it larger
    # Note the arguments for the objects have to be entered
    for j in range(total_no_of_iterations):
        epsilon = max(0, (1 - j/(int(total_no_of_iterations/2))))
        if j == 0:
            for i in range(no_of_agents):
                agent.append({'index': i, 'name': names[i],
                              'D_S_Actor': D_S_Actor(5, 0.0001, 0.5, clip_1, actions_log, max_battery, D_S_constraints[i]), 'D_S_Critic': D_S_Critic(5, 1),
                              'A_R_Actor': A_R_Actor((3 + 3 * no_of_agents), 0.0001, 0.5, no_of_agents, clip, A_R_constraints[i],  actions_log),
                              'P_Q_Actor': P_Q_Actor((3 + 5 * no_of_agents), 0.0001, 0.5, no_of_agents, clip, grid_price, lower_price, P_Q_constraints[i], actions_log),
                              'A_R_P_Q_Critic': A_R_P_Q_Critic((3 + 3 * no_of_agents), (3 + 5 * no_of_agents),
                                                               no_of_agents, no_of_agents, no_of_agents, actions_log),
                              'Total_Reward': 0,
                              'Battery': 0, 'Renewable': 0,
                              'Demand': 0, 'ADL_Value': 0, 'ADL_State': [], 'ADL_Action': [], 'D_S_State': [],
                              'D_S_Action': [], 'Current_Demand': 0, 'Main_Reward': 0,
                              'A_R_State': [], 'A_R_Action': [], 'P_Q_State': [], 'P_Q_Action': [], 'Via_Matrix': 0,
                              'Sub_Reward': 0, 'A_R_Vector_Others': np.zeros(no_of_agents),
                              'A_R_Vector': np.zeros(no_of_agents), 'P_Vector': np.zeros(no_of_agents),
                              'Q_Vector': np.zeros(no_of_agents), 'Status': [], 'Customised_Status': [],
                              'D_S_Buffer': ReplayBuffer(1, buffer_size),
                              'A_R_P_Q_Buffer': ReplayBuffer_A_R_P_Q(no_of_agents, no_of_agents, no_of_agents, 1, buffer_size)})

        '''
            Ignore ADL for now
            Hence the Demand state is: [time, battery, renewable-energy, current-demand, grid-price]  
        '''

        '''
                Difference between demand and current-demand is that current demand is updated over time, wheras demand stays constant
                Difference between Status and Customised-Status is that status is customized particularly for a buyer or a seller 
                Via_Matrix stands for Viability (Whether the transactions are valid or not) 
        '''
        buyers = []
        sellers = []
        nothing = []
        status = []  # Whether the agents are buyers or sellers or nothing (1: Sellers, 0:Buyers, -1: Nothing)

        # Getting the D_S_State and storing it in respective variables and replay buffers
        for i in range(no_of_agents):
            agent[i]['D_S_State'] = []
            agent[i]['D_S_Action'] = 0
            agent[i]['Main_Reward'] = 0
            agent[i]['Sub_Reward'] = 0
            agent[i]['A_R_State'] = []
            agent[i]['P_Q_State'] = []
            agent[i]['A_R_Vector'] = np.zeros(no_of_agents)
            agent[i]['Q_Vector'] = np.zeros(no_of_agents)
            agent[i]['P_Vector'] = np.zeros(no_of_agents)

            if i < 2:
                agent[i]['Renewable'] = get_renewable(i, j % time_steps_per_day)
                agent[i]['Demand'] = get_demand(i, j % time_steps_per_day)
            else:
                agent[i]['Renewable'] = agent[1]['Renewable']
                agent[i]['Demand'] = agent[1]['Demand']

            lower_bound = max(-1 * max_battery, (agent[i]['Renewable'] + agent[i]['Battery'] - agent[i]['Demand'] - max_battery))
            upper_bound = agent[i]['Renewable'] + agent[i]['Battery']
            agent[i]['D_S_State'] = [j % time_steps_per_day, agent[i]['Battery'], agent[i]['Renewable'],
                                     agent[i]['Demand'], grid_price]

            agent[i]['D_S_Buffer'].store_transition_state(agent[i]['D_S_State'])
            agent[i]['D_S_State'] = np.asarray([[j % time_steps_per_day, agent[i]['Battery'], agent[i]['Renewable'],
                                                 agent[i]['Demand'], grid_price]])

        # Getting the D_S_Actions and storing it in respective variables and replay buffers using the constraints
        for i in range(no_of_agents):


            lower_bound = max(-1 * max_battery, (agent[i]['Renewable'] + agent[i]['Battery'] - agent[i]['Demand'] - max_battery))
            upper_bound = agent[i]['Renewable'] + agent[i]['Battery']

            agent[i]['D_S_Action'] = agent[i]['D_S_Actor'].action(agent[i]['D_S_State'], epsilon)
            agent[i]['Current_Demand'] = agent[i]['D_S_Action'][0][0] * (upper_bound - lower_bound) / (2 * clip_1) + (upper_bound + lower_bound) / 2
            agent[i]['D_S_Buffer'].store_transition_action(agent[i]['D_S_Action'])


            agent[i]['Main_Reward'] = c * min(0,
                                              (agent[i]['Renewable'] + agent[i]['Battery'] - agent[i]['Current_Demand'] -
                                              agent[i]['Demand']))
            agent[i]['Battery'] = max(0, min((
                    agent[i]['Renewable'] + agent[i]['Battery'] - agent[i]['Current_Demand'] - agent[i]['Demand']), max_battery))


            if agent[i]['Current_Demand'] < 0:
                status.append(0)
            elif agent[i]['Current_Demand'] == 0:
                status.append(-1)
            else:
                status.append(1)

        # Customized Status
        for i in range(no_of_agents):
            agent[i]['Status'] = status
            status_1 = copy.deepcopy(status)
            if agent[i]['Current_Demand'] < 0:
                for l in range(len(status)):
                    if status_1[l] == (-1):
                        status_1[l] = 0
            elif agent[i]['Current_Demand'] == 0:
                pass
            else:
                for l in range(len(status)):
                    if status_1[l] == (-1):
                        status_1[l] = 1
            agent[i]['Customised_Status'] = copy.deepcopy(np.array(status_1))

        # Separating into Buyers, Sellers or Nothing and updating the Via_Matrix
        for i in range(no_of_agents):
            if agent[i]['Current_Demand'] < 0:
                agent[i]['Via_Matrix'] = np.diag(agent[i]['Customised_Status'])
                buyers.append(agent[i])
            elif agent[i]['Current_Demand'] == 0:
                agent[i]['Via_Matrix'] = np.diag(agent[i]['Customised_Status'])
                nothing.append(agent[i])
            else:
                agent[i]['Via_Matrix'] = np.eye(len(agent[i]['Customised_Status'])) - np.diag(
                    agent[i]['Customised_Status'])
                agent[i]['Customised_Status'] = 1 - np.array(agent[i]['Customised_Status'])
                sellers.append(agent[i])

        energy_transfered.info("Iteration:{}".format(j))
        for i in range(no_of_agents):
            main_transaction.info('Time Step: {}'.format(j))
            main_transaction.info('[[time, battery, renewable-energy, current-demand, grid-price]]')
            main_transaction.info(
                "Agent: {} | D_S_State: {} | Status: {} | Action: {}".format(agent[i]['index'], agent[i]['D_S_State'],
                                                                             agent[i]['Customised_Status'],
                                                                             agent[i]['Current_Demand']))
            energy_transfered.info("Agent: {} | Demand: {}". format(agent[i]['index'], agent[i]['Current_Demand']))
            energy_demand[i] += abs(agent[i]['Current_Demand'])
        main_transaction.info(' ')
        '''
        States and Actions of the Sub-Controllers (Roughly):
            A_R:
                States: [ internal time, demand, current-demand, status-vector, p-vector for all the other agents, q-vector for all the other agents]
                Actions: Accept or Reject Vector
            P_Q:
                States:  [ internal time, demand, current-demand, status-vector, p-vector for all the other agents, q-vector for all the other agents, A_R_Vector]
                Actions: Price and Quantity Vector
        '''

        # Sub_Transactions
        for k in range(no_of_sub_transactions):

            for m in range(len(sellers)):
                sellers[m]['A_R_P_Q_Buffer'].store_transition_via_matrix(sellers[m]['Via_Matrix'])
                sellers[m]['A_R_P_Q_Buffer'].store_transition_customized_status(sellers[m]['Customised_Status'])

            for m in range(len(buyers)):
                buyers[m]['A_R_P_Q_Buffer'].store_transition_via_matrix(buyers[m]['Via_Matrix'])
                buyers[m]['A_R_P_Q_Buffer'].store_transition_customized_status(buyers[m]['Customised_Status'])

            # A_R for Sellers
            # If the Sub-Transaction time is 0, Ignore the A_R_Network, the action would be a zero everywhere
            sub_transaction.info("Global Time: {} | Local Time : {}".format(j, k))
            sub_transaction.info(' ')
            sub_transaction.info('Sellers A_R')
            actions_log.info('-----------')
            actions_log.info("Global Time: {} | Local Time : {}".format(j, k))

            # All the P and Q Values of the buyers have to made 0. The P_Vector and the Q_Vector denotes the P and Q qouted
            # by the other sellers. We reset the
            for l in range(len(buyers)):
                buyers[l]['P_Vector'] = np.zeros(no_of_agents)
                buyers[l]['Q_Vector'] = np.zeros(no_of_agents)
                buyers[l]['A_R_Vector_Others'] = np.zeros(no_of_agents)

            if k == 0:
                for m in range(len(sellers)):
                    sellers[m]['A_R_State'] = [k, sellers[m]['Demand'], sellers[m]['Current_Demand']]
                    sellers[m]['A_R_State'].extend(sellers[m]['Customised_Status'])
                    sellers[m]['A_R_State'].extend(sellers[m]['P_Vector'])
                    sellers[m]['A_R_State'].extend(sellers[m]['Q_Vector'])
                    sellers[m]['P_Q_State'] = copy.deepcopy(sellers[m]['A_R_State'])
                    # print(sellers[m]['P_Vector'])
                    # print(sellers[m]['Q_Vector'])
                    sellers[m]['A_R_P_Q_Buffer'].store_transition_state_a_r(sellers[m]['A_R_State'])
                    # print(sellers[m]['P_Q_State'])
                    # print(len(sellers[m]['P_Q_State']))
                    sellers[m]['A_R_P_Q_Buffer'].store_transition_action_a_r(np.zeros(no_of_agents))

                    sellers[m]['P_Q_State'].extend(np.zeros(no_of_agents))  # For the A_R_Action
                    sellers[m]['P_Q_State'].extend(np.zeros(no_of_agents))  # For A_R_Action of others
                    sellers[m]['A_R_P_Q_Buffer'].store_transition_state_p_q(sellers[m]['P_Q_State'])
                    sellers[m]['P_Q_State'] = np.asarray([sellers[m]['P_Q_State']])

                    sub_transaction.info(
                        'Agent: {} | A_R_State: {} | A_R_Action: {} | Current Demand: {}'.format(
                            sellers[m]['index'], sellers[m]['A_R_State'], sellers[m]['A_R_Vector'], sellers[m]['Current_Demand']))

            else:
                for m in range(len(sellers)):
                    sellers[m]['A_R_State'] = [k, sellers[m]['Demand'], sellers[m]['Current_Demand']]
                    sellers[m]['A_R_State'].extend(sellers[m]['Customised_Status'])
                    sellers[m]['A_R_State'].extend((sellers[m]['P_Vector']))
                    sellers[m]['A_R_State'].extend((sellers[m]['Q_Vector']))

                    sellers[m]['A_R_P_Q_Buffer'].store_transition_state_a_r(sellers[m]['A_R_State'])
                    sellers[m]['A_R_State'] = np.asarray([sellers[m]['A_R_State']])

                    sellers[m]['A_R_Action'] = sellers[m]['A_R_Actor'].action(sellers[m]['A_R_State'], epsilon)
                    sellers[m]['A_R_Action'] = np.matmul(sellers[m]['Via_Matrix'], sellers[m]['A_R_Action'][0])
                    sellers[m]['A_R_Vector'] = (np.asarray(sellers[m]['A_R_Action']) > 0.5) * 1

                    for n in range(len(buyers)):
                        buyers[n]['A_R_Vector_Others'][sellers[m]['index']] = sellers[m]['A_R_Vector'][buyers[n]['index']]


                    sellers[m]['P_Q_State'] = list(copy.deepcopy(sellers[m]['A_R_State'][0]))
                    sellers[m]['P_Q_State'].extend(sellers[m]['A_R_Action'])
                    sellers[m]['P_Q_State'].extend(sellers[m]['A_R_Vector_Others'])

                    sellers[m]['A_R_P_Q_Buffer'].store_transition_action_a_r(sellers[m]['A_R_Action'])

                    total_q = 0
                    reward = 0

                    sub_transaction.info(
                        'Agent: {} | A_R_State: {} | A_R_Action: {} | Current Demand: {}'.format(
                            sellers[m]['index'], sellers[m]['A_R_State'], sellers[m]['A_R_Vector'],
                            sellers[m]['Current_Demand']))

                    for n in range(len(buyers)):
                        # The P and Q vector of the Sellers have to be the P and Q values of each buyer. That means that
                        # the  each P would be positive and each Q would be negative
                        buyers[n]['Sub_Reward'] = buyers[n]['Sub_Reward'] + sellers[m]['Q_Vector'][buyers[n]['index']] * \
                                                  sellers[m]['P_Vector'][buyers[n]['index']] * \
                                                  sellers[m]['A_R_Vector'][buyers[n]['index']]


                        # The negative sign makes sense as the values of the Q_vector are negative and the current demand
                        # is positive
                        buyers[n]['Current_Demand'] = min(0, (buyers[n]['Current_Demand'] - sellers[m]['Q_Vector'][
                            buyers[n]['index']] * sellers[m]['A_R_Vector'][buyers[n]['index']]))

                        # The negative sign makes sense as the values of the Q_vector are negative
                        total_q = total_q - sellers[m]['Q_Vector'][buyers[n]['index']] * sellers[m]['A_R_Vector'][
                            buyers[n]['index']]

                        # The negative sign makes sense as the values of the Q_vector are negative
                        reward = reward - sellers[m]['Q_Vector'][buyers[n]['index']] * sellers[m]['P_Vector'][
                            buyers[n]['index']] * sellers[m]['A_R_Vector'][buyers[n]['index']]


                    if total_q > sellers[m]['Current_Demand']:
                        # The reward is decreased as the excess energy sold is bought from the main grid.
                        reward = reward - (total_q - sellers[m]['Current_Demand']) * grid_price
                        sellers[m]['Sub_Reward'] = sellers[m]['Sub_Reward'] + reward
                        sellers[m]['Current_Demand'] = 0

                    else:
                        sellers[m]['Sub_Reward'] = sellers[m]['Sub_Reward'] + reward
                        sellers[m]['Current_Demand'] = sellers[m]['Current_Demand'] - total_q

                    sellers[m]['P_Q_State'][2] = sellers[m]['Current_Demand']  # Updating the Current Demand after A_R
                    sellers[m]['A_R_P_Q_Buffer'].store_transition_state_p_q(sellers[m]['P_Q_State'])
                    sellers[m]['P_Q_State'] = np.asarray([sellers[m]['P_Q_State']])

            sub_transaction.info(' ')
            sub_transaction.info('Sellers_P_Q')

            # P_Q_Action for Sellers

            for m in range((len(sellers))):

                action_p, action_q, action_q_traded = sellers[m]['P_Q_Actor'].action(sellers[m]['P_Q_State'], epsilon)

                action_p = np.matmul(sellers[m]['Via_Matrix'], action_p[0])

                # Done to not get nan values, 0/0
                normalisation = np.matmul(np.matmul(sellers[m]['Via_Matrix'], action_q[0]), sellers[m]['Customised_Status'])
                if normalisation:
                    action_q = np.matmul(sellers[m]['Via_Matrix'], action_q[0]) / normalisation
                else:
                    action_q = np.matmul(sellers[m]['Via_Matrix'], action_q[0])

                action_q_traded = action_q_traded[0]

                sellers[m]['A_R_P_Q_Buffer'].store_transition_action_p_q(action_p, action_q, action_q_traded)

                action_p = action_p * (grid_price - lower_price) / (2 * clip) + (grid_price + lower_price) / 2
                action_q = action_q_traded * action_q * sellers[m]['Current_Demand'] / (2 * clip) + action_q * sellers[m]['Current_Demand'] / 2
                action_p = np.matmul(sellers[m]['Via_Matrix'], action_p)
                action_q = np.matmul(sellers[m]['Via_Matrix'], action_q)
                sub_transaction.info("Agent: {} | P_Q_State: {} | Actions_P: {} | Actions_Q: {} | Actions_Q_Traded: {}".format(sellers[m]['index'], sellers[m]['P_Q_State'], action_p, action_q, action_q_traded))
                if sellers[m]['index'] == 2:
                    price_log.info("{}, {}".format(action_p, action_q))


                for n in range(len(buyers)):
                    buyers[n]['P_Vector'][sellers[m]['index']] = action_p[buyers[n]['index']]
                    buyers[n]['Q_Vector'][sellers[m]['index']] = action_q[buyers[n]['index']]
            sub_transaction.info(' ')

            # For resetting all the P, Q as well as the A_R_Vector_Others
            for l in range(len(sellers)):
                sellers[l]['P_Vector'] = np.zeros(no_of_agents)
                sellers[l]['Q_Vector'] = np.zeros(no_of_agents)
                sellers[l]['A_R_Vector_Others'] = np.zeros(no_of_agents)

            # A_R_Action for Buyers
            sub_transaction.info(' ')
            sub_transaction.info('Buyers_A_R')

            for m in range(len(buyers)):
                buyers[m]['A_R_State'] = [k, buyers[m]['Demand'], buyers[m]['Current_Demand']]
                buyers[m]['A_R_State'].extend(buyers[m]['Customised_Status'])
                buyers[m]['A_R_State'].extend((buyers[m]['P_Vector']))
                buyers[m]['A_R_State'].extend((buyers[m]['Q_Vector']))

                buyers[m]['A_R_P_Q_Buffer'].store_transition_state_a_r(buyers[m]['A_R_State'])
                buyers[m]['A_R_State'] = np.asarray([buyers[m]['A_R_State']])

                buyers[m]['A_R_Action'] = buyers[m]['A_R_Actor'].action(buyers[m]['A_R_State'], epsilon)
                buyers[m]['A_R_Action'] = np.matmul(buyers[m]['Via_Matrix'], buyers[m]['A_R_Action'][0])
                buyers[m]['A_R_Vector'] = (np.asarray(buyers[m]['A_R_Action']) > 0.5) * 1

                for n in range(len(sellers)):
                    sellers[n][buyers[m]['index']] = buyers[m]['A_R_Vector'][sellers[n]['index']]

                buyers[m]['P_Q_State'] = list(copy.deepcopy(buyers[m]['A_R_State'][0]))
                buyers[m]['P_Q_State'].extend(buyers[m]['A_R_Action'])
                buyers[m]['P_Q_State'].extend(buyers[m]['A_R_Vector_Others'])

                buyers[m]['A_R_P_Q_Buffer'].store_transition_action_a_r(buyers[m]['A_R_Action'])

                total_q = 0
                reward = 0

                sub_transaction.info(
                    'Agent: {} | A_R_State: {} | A_R_Action: {} | Current Demand: {}'.format(
                        buyers[m]['index'], buyers[m]['A_R_State'], buyers[m]['A_R_Vector'], buyers[m]['Current_Demand']))

                for n in range(len(sellers)):
                    # The P and Q vector of the buyers have to be the P and Q values of each seller. That means that
                    # the  each P would be positive and each Q would be positive

                    sellers[n]['Sub_Reward'] = sellers[n]['Sub_Reward'] + buyers[m]['Q_Vector'][sellers[n]['index']] * \
                                               buyers[m]['P_Vector'][sellers[n]['index']] * \
                                               buyers[m]['A_R_Vector'][sellers[n]['index']]

                    # The negative sign makes sense as the values of the Q_vector are positive and the current demand
                    # is positive
                    sellers[n]['Current_Demand'] = max(0, (sellers[n]['Current_Demand'] - buyers[m]['Q_Vector'][
                        sellers[n]['index']] * buyers[m]['A_R_Vector'][sellers[n]['index']]))

                    total_q = total_q + buyers[m]['Q_Vector'][sellers[n]['index']] * buyers[m]['A_R_Vector'][
                        sellers[n]['index']]

                    # The negative sign makes sense as the values of the Q_vector are positive
                    reward = reward - buyers[m]['Q_Vector'][sellers[n]['index']] * buyers[m]['P_Vector'][
                        sellers[n]['index']] * \
                             buyers[m]['A_R_Vector'][sellers[n]['index']]

                if (-total_q) < buyers[m]['Current_Demand']:
                    # buyers[m]['Current_Demand'] is negative and total_q is positive. Reward is increased as the excess
                    # energy is sold back to the main grid.
                    reward = reward + (total_q + buyers[m]['Current_Demand']) * lower_price
                    buyers[m]['Sub_Reward'] = buyers[m]['Sub_Reward'] + reward
                    buyers[m]['Current_Demand'] = 0

                else:
                    buyers[m]['Sub_Reward'] = buyers[m]['Sub_Reward'] + reward
                    buyers[m]['Current_Demand'] = buyers[m]['Current_Demand'] + total_q

                buyers[m]['P_Q_State'][2] = buyers[m]['Current_Demand']
                buyers[m]['A_R_P_Q_Buffer'].store_transition_state_p_q(buyers[m]['P_Q_State'])
                buyers[m]['P_Q_State'] = np.asarray([buyers[m]['P_Q_State']])


            sub_transaction.info(' ')
            sub_transaction.info('Buyers_P_Q')

            # P_Q_Action for buyers. If its the last sub time step, all the p_actions and q_actions are 0 and there is
            # no need of updating the sellers

            for m in range(len(buyers)):

                if k == (no_of_sub_transactions - 1):
                    buyers[m]['A_R_P_Q_Buffer'].store_transition_action_p_q(np.zeros(no_of_agents),
                                                                            np.zeros(no_of_agents), np.zeros(1))
                    action_p = np.array([np.zeros(no_of_agents)])
                    action_q = np.array([np.zeros(no_of_agents)])
                    action_q_traded = np.array([np.zeros(1)])

                else:
                    action_p, action_q, action_q_traded = buyers[m]['P_Q_Actor'].action(buyers[m]['P_Q_State'], epsilon)

                action_p = np.matmul(buyers[m]['Via_Matrix'], action_p[0])

                # Done to not get nan values, 0/0
                normalisation = np.matmul(np.matmul(buyers[m]['Via_Matrix'], action_q[0]), buyers[m]['Customised_Status'])
                if normalisation:
                    action_q = np.matmul(buyers[m]['Via_Matrix'], action_q[0]) / normalisation
                else:
                    action_q = np.matmul(buyers[m]['Via_Matrix'], action_q[0])

                action_q_traded = action_q_traded[0]

                buyers[m]['A_R_P_Q_Buffer'].store_transition_action_p_q(action_p, action_q, action_q_traded)

                action_p = action_p * (grid_price - lower_price) / (2 * clip) + (grid_price + lower_price) / 2
                action_q = action_q_traded * action_q * buyers[m]['Current_Demand'] / (2 * clip) + action_q * buyers[m]['Current_Demand'] / 2
                action_p = np.matmul(buyers[m]['Via_Matrix'], action_p)
                action_q = np.matmul(buyers[m]['Via_Matrix'], action_q)

                sub_transaction.info(
                    "Agent: {} | P_Q_State: {} | Actions_P: {} | Actions_Q: {} | Actions_Q_Traded: {}".format(
                        buyers[m]['index'], buyers[m]['P_Q_State'], action_p, action_q, action_q_traded))

                if buyers[m]['index'] == 2:
                    price_log.info("{}, {}".format(action_p, action_q))

                for n in range(len(sellers)):
                    sellers[n]['P_Vector'][buyers[m]['index']] = action_p[sellers[n]['index']]
                    sellers[n]['Q_Vector'][buyers[m]['index']] = action_q[sellers[n]['index']]

            sub_transaction.info(' ')

            if k == (no_of_sub_transactions - 1):
                energy_transfered.info(' ')

            for m in range(no_of_agents):
                if k == (no_of_sub_transactions -1):
                    energy_transfered.info("Agent: {} | Demand: {}".format(agent[m]['index'], agent[m]['Current_Demand']))
                    energy_transacted_central[m] += abs(agent[m]['Current_Demand'])

            if k == (no_of_sub_transactions - 1):
                energy_transfered.info('-----------------------------------------------')

            for m in range(len(sellers)):
                if k == (no_of_sub_transactions - 1):
                    sellers[m]['Sub_Reward'] += sellers[m]['Current_Demand'] * lower_price
                    sellers[m]['A_R_P_Q_Buffer'].store_transition_reward(sellers[m]['Sub_Reward'])
                else:
                    sellers[m]['A_R_P_Q_Buffer'].store_transition_reward(0)

            for m in range(len(buyers)):
                if k == (no_of_sub_transactions - 1):
                    buyers[m]['Sub_Reward'] += buyers[m]['Current_Demand'] * grid_price
                    buyers[m]['A_R_P_Q_Buffer'].store_transition_reward(buyers[m]['Sub_Reward'])
                else:
                    buyers[m]['A_R_P_Q_Buffer'].store_transition_reward(0)

        sub_transaction.info('-------------------------------------------------------------------------------')
        for i in range(no_of_agents):
            main_transaction.info("Agent {} | Penalty: {} | Sub_Reward: {}".format(agent[i]['index'], agent[i]['Main_Reward'], agent[i]['Sub_Reward']))
            agent[i]['Main_Reward'] += agent[i]['Sub_Reward']
            agent[i]['D_S_Buffer'].store_transition_reward(agent[i]['Main_Reward'])
            agent[i]['Total_Reward'] = agent[i]['Total_Reward'] + agent[i]['Main_Reward']

            if (j % 1000) == 0:
                reward_logger.info("Iteration: {} | Agent: {} | Main_Reward: {} | Energy_DS: {} | Energy_Bst: {}".format(j, agent[i]['index'], (agent[i]['Total_Reward']/1000), (energy_demand[i]/1000), (energy_transacted_central[i]/1000)))
                agent[i]['Total_Reward'] = 0
                energy_demand[i] = 0
                energy_transacted_central[i] = 0

        if (j % 1000) == 0:
            reward_logger.info('--------------------------------------')
            reward_logger.info(' ')
        main_transaction.info(' ')
        main_transaction.info('-----------------------------------------------------------')

        # Training
        print("Iteration Number:", j)
        if ((j > buffer_size) and (j < total_no_of_iterations - buffer_size)):
            losses_logger.info("Number: {}".format(j))
            batch_size = 4
            for i in range(no_of_agents):
                if j % batch_size == 0:


                    # if (j % 1000) < 200:
                    #     i = 0
                    # elif ((j % 1000) > 200) and ((j % 1000) < 400):
                    #     i = 1
                    # elif ((j % 1000) > 400) and ((j % 1000) < 600):
                    #     i = 2
                    # elif ((j % 1000) > 600) and ((j % 1000) < 800):
                    #     i = 3
                    # elif ((j % 1000) > 800) and ((j % 1000) < 1000):
                    #     i = 4


                    # Training the D_S Actor Critic first
                    if agent[i]['D_S_Actor'].constraint_value() == 0:
                        current_states, actions, rewards, next_states, next_actions = agent[i]['D_S_Buffer'].sample_buffer(
                            batch_size)
                        # Critic Training

                        loss_D_S_Critic = agent[i]['D_S_Critic'].train(current_states, actions, next_states, next_actions,
                                                                       rewards)
                        losses_logger.info('Agent: {} | D_S_Critic: {}'.format(agent[i]['index'], loss_D_S_Critic))

                        losses_logger.info(' ')
                        # Actor Training

                        actions = agent[i]['D_S_Actor'].action(current_states, epsilon)
                        grads = agent[i]['D_S_Critic'].gradients(current_states, actions)
                        grads = np.reshape(grads, (-1, 1))
                        loss_D_S_Actor = agent[i]['D_S_Actor'].train(current_states, grads)
                        losses_logger.info('Agent: {} | D_S_Actor: {}'.format(agent[i]['index'], loss_D_S_Actor))

                    # Training the A_R P_Q Actor and A_R_P_Q_Critic
                    current_states_a_r, current_states_p_q, actions_a_r, actions_p, actions_q, actions_q_traded, rewards, next_states_a_r, \
                    next_states_p_q, next_actions_a_r, next_actions_p, next_actions_q, next_actions_q_traded, via_matrix, customized_status, \
                    next_via_matrix, next_customized_status = agent[i]['A_R_P_Q_Buffer'].sample_buffer(batch_size)

                    losses_logger.info(' ')
                    # Critic Training
                    rewards = np.asarray([rewards])


                    loss_A_R_P_Q_Critic = agent[i]['A_R_P_Q_Critic'].train(current_states_a_r, current_states_p_q,
                                                                           actions_a_r, actions_p, actions_q,
                                                                           actions_q_traded, next_states_a_r,
                                                                           next_states_p_q, next_actions_a_r,
                                                                           next_actions_p, next_actions_q,
                                                                           next_actions_q_traded, rewards)
                    losses_logger.info('Agent: {} | A_R_P_Q_Critic: {}'.format(agent[i]['index'], loss_A_R_P_Q_Critic))
                    # Actor Training

                    actions_a_r = agent[i]['A_R_Actor'].action(current_states_a_r, epsilon)
                    actions_p, actions_q, actions_q_traded = agent[i]['P_Q_Actor'].action(current_states_p_q, epsilon)

                    for p in range(batch_size):
                        actions_a_r[p] = np.matmul(via_matrix[p], actions_a_r[p])
                        actions_p[p] = np.matmul(via_matrix[p], actions_p[p])
                        normalisation = np.matmul(np.matmul(via_matrix[p], actions_q[p]), customized_status[p])
                        if normalisation:
                            actions_q[p] = np.matmul(via_matrix[p], actions_q[p]) / normalisation
                        else:
                            actions_q[p] = np.matmul(via_matrix[p], actions_q[p])

                    # actions_q = np.matmul(actions_q, via_matrix) / np.matmul(customized_status,
                    #                                                          np.matmul(actions_q, via_matrix))

                    grads_a_r, grads_p, grads_q, grads_q_traded = agent[i]['A_R_P_Q_Critic'].gradients(current_states_a_r,
                                                                                                       current_states_p_q,
                                                                                                       actions_a_r,
                                                                                                       actions_p, actions_q,
                                                                                                       actions_q_traded)

                    if agent[i]['A_R_Actor'].constraint_value() > 0:
                        grads_a_r = np.zeros_like(grads_a_r)

                    if agent[i]['P_Q_Actor'].constraint_value() > 0:
                        grads_p = np.zeros_like(grads_p)
                        grads_q = np.zeros_like(grads_q)
                        grads_q_traded = np.zeros_like(grads_q_traded)

                    grads_a_r = np.reshape(grads_a_r, (-1, no_of_agents))
                    grads_p = np.reshape(grads_p, (-1, no_of_agents))
                    grads_q_traded = np.reshape(grads_q_traded, (-1, 1))

                    loss_A_R_Actor = agent[i]['A_R_Actor'].train(current_states_a_r, grads_a_r)
                    loss_P_Q_Actor = agent[i]['P_Q_Actor'].train(current_states_p_q, grads_p, grads_q, grads_q_traded)

                    losses_logger.info('Agent: {} | A_R_Actor: {} | P_Q_Actor: {}'.format(agent[i]['index'], loss_A_R_Actor, loss_P_Q_Actor))

                    # losses_logger.info("Agent: {} | D_S_Critic: {} | D_S_Actor: {} | A_R_P_Q_Critic: {} | A_R_Actor: {} | P_Q_Actor: {}".format(agent[i]['index'],loss_D_S_Critic, loss_D_S_Actor, loss_A_R_P_Q_Critic, loss_A_R_Actor, loss_P_Q_Actor))

                if (j % 16) == 0:
                    losses_logger.info('-----------------------------------------------------------------')


if __name__ == "__main__":
    main()
